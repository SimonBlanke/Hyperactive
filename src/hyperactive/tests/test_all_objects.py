"""Automated tests based on the skbase test suite template."""

from inspect import isclass

from skbase.testing import BaseFixtureGenerator as _BaseFixtureGenerator
from skbase.testing import QuickTester as _QuickTester
from skbase.testing import TestAllObjects as _TestAllObjects

from hyperactive._registry import all_objects
from hyperactive.tests._config import EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
from hyperactive.tests._doctest import run_doctest

# whether to test only estimators from modules that are changed w.r.t. main
# default is False, can be set to True by pytest --only_changed_modules True flag
ONLY_CHANGED_MODULES = False


class PackageConfig:
    """Contains package config variables for test classes."""

    # class variables which can be overridden by descendants
    # ------------------------------------------------------

    # package to search for objects
    # expected type: str, package/module name, relative to python environment root
    package_name = "hyperactive"

    # list of object types (class names) to exclude
    # expected type: list of str, str are class names
    exclude_objects = EXCLUDE_ESTIMATORS

    # list of tests to exclude
    # expected type: dict of lists, key:str, value: List[str]
    # keys are class names of estimators, values are lists of test names to exclude
    excluded_tests = EXCLUDED_TESTS

    # list of valid tags
    # expected type: list of str, str are tag names
    valid_tags = [
        # general tags
        "object_type",
        "python_dependencies",
        "authors",
        "maintainers",
        # experiments
        "property:randomness",
        "property:higher_or_lower_is_better",
        # optimizers
        "info:name",  # str
        "info:local_vs_global",  # "local", "mixed", "global"
        "info:explore_vs_exploit",  # "explore", "exploit", "mixed"
        "info:compute",  # "low", "middle", "high"
    ]


class BaseFixtureGenerator(PackageConfig, _BaseFixtureGenerator):
    """Fixture generator for base testing functionality in sktime.

    Test classes inheriting from this and not overriding pytest_generate_tests
        will have estimator and scenario fixtures parametrized out of the box.

    Descendants can override:
    object_type_filter: str, class variable; None or scitype string
        e.g., "forecaster", "transformer", "classifier", see BASE_CLASS_SCITYPE_LIST
        which objects are being retrieved and tested
    exclude_objects : str or list of str, or None, default=None
        names of object classes to exclude in retrieval; None = no objects are excluded
    excluded_tests : dict with str keys and list of str values, or None, default=None
        str keys must be object names, value keys must be lists of test names
        names of tests (values) to exclude for object with name as key
        None = no tests are excluded
    valid_tags : list of str or None, default = None
        list of valid tags, None = all tags are valid
    valid_base_types : list of str or None, default = None
        list of valid base types (strings), None = all base types are valid
    fixture_sequence: list of str
        sequence of fixture variable names in conditional fixture generation
    _generate_[variable]: object methods, all (test_name: str, **kwargs) -> list
        generating list of fixtures for fixture variable with name [variable]
            to be used in test with name test_name
        can optionally use values for fixtures earlier in fixture_sequence,
            these must be input as kwargs in a call
    is_excluded: static method (test_name: str, est: class) -> bool
        whether test with name test_name should be excluded for object est
            should be used only for encoding general rules, not individual skips
            individual skips should go on the excluded_tests list
        requires _generate_object_class and _generate_object_instance as is

    Fixtures parametrized
    ---------------------
    object_class: class inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
    object_instance: object instances inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
        instances are generated by create_test_instance class method of object_class
    """

    # overrides object retrieval in scikit-base
    def _all_objects(self):
        """Retrieve list of all object classes of type self.object_type_filter.

        If self.object_type_filter is None, retrieve all objects.
        If class, retrieve all classes inheriting from self.object_type_filter.
        Otherwise (assumed str or list of str), retrieve all classes with tags
        object_type in self.object_type_filter.
        """
        filter = getattr(self, "object_type_filter", None)

        if isclass(filter):
            object_types = filter.get_class_tag("object_type", None)
        else:
            object_types = filter

        obj_list = all_objects(
            object_types=object_types,
            return_names=False,
            exclude_objects=self.exclude_objects,
        )

        if isclass(filter):
            obj_list = [obj for obj in obj_list if issubclass(obj, filter)]

        # only run tests if all soft dependencies are present
        def softdeps_present(obj):
            """Check if the object has all dependencies present."""
            from skbase.utils.dependencies import _check_estimator_deps

            return _check_estimator_deps(obj, severity="none")

        obj_list = [obj for obj in obj_list if softdeps_present(obj)]

        return obj_list

    # which sequence the conditional fixtures are generated in
    fixture_sequence = ["object_class", "object_instance"]


class TestAllObjects(BaseFixtureGenerator, _TestAllObjects):
    """Generic tests for all objects in the package."""

    OBJECT_TYPES_IN_HYPERACTIVE = [
        "experiment",
        "optimizer",
    ]

    def test_doctest_examples(self, object_class):
        """Runs doctests for estimator class."""
        run_doctest(object_class, name=f"class {object_class.__name__}")

    def test_valid_object_class_tags(self, object_class):
        """Check that object class tags are in self.valid_tags."""
        # stepout for estimators with base classes in other packages
        # e.g., sktime BaseForecaster, BaseClassifier, used in hyperactive.integrations
        cls_type = object_class.get_class_tag("object_type", None)
        if cls_type not in self.OBJECT_TYPES_IN_HYPERACTIVE:
            return None

        super().test_valid_object_class_tags(object_class)

    def test_valid_object_tags(self, object_instance):
        """Check that object tags are in self.valid_tags."""
        # stepout for estimators with base classes in other packages
        # e.g., sktime BaseForecaster, BaseClassifier, used in hyperactive.integrations
        obj_type = object_instance.get_tag("object_type", None)
        if obj_type not in self.OBJECT_TYPES_IN_HYPERACTIVE:
            return None

        super().test_valid_object_class_tags(object_instance)


class ExperimentFixtureGenerator(BaseFixtureGenerator):
    """Fixture generator for experiments.

    Fixtures parameterized
    ----------------------
    object_class: class inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
    object_instance: object instances inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
        instances are generated by create_test_instance class method of object_class
    """

    object_type_filter = "experiment"


class TestAllExperiments(ExperimentFixtureGenerator, _QuickTester):
    """Module level tests for all experiment classes."""

    def test_paramnames(self, object_class):
        """Test that paramnames returns the correct parameter names."""
        inst_params = object_class.get_test_params()
        obj_params = object_class._get_score_params()

        for inst, obj_param in zip(inst_params, obj_params):
            obj_inst = object_class(**inst)
            paramnames = obj_inst.paramnames()
            if paramnames is not None:
                assert set(obj_param.keys()) <= set(
                    paramnames
                ), f"Parameter names do not match: {paramnames} != {obj_param}"

    def test_score_function(self, object_class):
        """Test that substituting into score works as intended."""
        inst_params = object_class.get_test_params()
        obj_params = object_class._get_score_params()

        for inst, obj in zip(inst_params, obj_params):
            inst = object_class(**inst)
            res = inst.score(obj)
            msg = f"Score function did not return a length two tuple: {res}"
            assert isinstance(res, tuple) and len(res) == 2, msg
            score, metadata = res
            assert isinstance(score, float), f"Score is not a float: {score}"
            assert isinstance(metadata, dict), f"Metadata is not a dict: {metadata}"

            eval_res = inst.evaluate(obj)
            msg = f"eval function did not return a length two tuple: {res}"
            assert isinstance(eval_res, tuple) and len(eval_res) == 2, msg
            e_score, e_metadata = eval_res
            assert isinstance(e_score, float), f"Score is not a float: {e_score}"
            assert isinstance(e_metadata, dict), f"Metadata is not a dict: {e_metadata}"

            det_tag = inst.get_tag("property:randomness", "random")

            if det_tag == "deterministic":
                msg = f"Score and eval calls do not match: |{e_score}| != |{score}|"
                assert abs(e_score) == abs(score), msg

            call_sc = inst(obj)
            assert isinstance(call_sc, float), f"Score is not a float: {call_sc}"
            if det_tag == "deterministic":
                msg = f"Score does not match: {score} != {call_sc}"
                assert score == call_sc, msg

            sign_tag = inst.get_tag("property:higher_or_lower_is_better", "higher")
            if sign_tag == "higher" and det_tag == "deterministic":
                assert score == e_score
            elif sign_tag == "lower" and det_tag == "deterministic":
                assert score == -e_score


class OptimizerFixtureGenerator(BaseFixtureGenerator):
    """Fixture generator for optimizers.

    Fixtures parameterized
    ----------------------
    object_class: class inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
    object_instance: object instances inheriting from BaseObject
        ranges over classes not excluded by EXCLUDE_ESTIMATORS, EXCLUDED_TESTS
        instances are generated by create_test_instance class method of object_class
    """

    object_type_filter = "optimizer"


class TestAllOptimizers(OptimizerFixtureGenerator, _QuickTester):
    """Module level tests for all optimizer classes."""

    def test_opt_run(self, object_instance):
        """Test that run returns the expected result."""
        paramnames = object_instance.get_params().keys()
        if "experiment" not in paramnames:
            raise ValueError("Optimizer must have an 'experiment' parameter.")
        # check that experiment occurs last in __init__ signature
        if not object_instance.__init__.__code__.co_varnames[-1] == "experiment":
            raise ValueError(
                "'experiment' parameter in optimizer __init__ must be last argument."
            )
        if not hasattr(object_instance, "experiment"):
            raise ValueError(
                "Optimizer test cases must have 'experiment' parameter defined."
            )
        experiment = object_instance.experiment
        msg = "experiment must be an instance of BaseExperiment."
        if not hasattr(experiment, "get_tag"):
            raise ValueError(msg)
        if not experiment.get_tag("object_type") == "experiment":
            raise ValueError(msg)

        best_params = object_instance.solve()

        assert isinstance(best_params, dict), "return of run is not a dict"

        paramnames = list(best_params.keys())
        expected_paramnames = experiment.paramnames()

        assert set(paramnames) <= set(expected_paramnames), (
            f"Optimizer run must return a dict with keys being paramnames "
            f"from the experiment, but found: {paramnames} !<= {expected_paramnames}"
        )

        msg = "Optimizer run must write best_params_ to self in run."
        if not hasattr(object_instance, "best_params_"):
            raise ValueError(msg)
        msg = "Optimizer best_params_ must equal the best_params returned by run."
        if not object_instance.best_params_ == best_params:
            raise ValueError(msg)

    def test_gfo_integration(self, object_instance):
        """Integration test for optimizer end-to-end, for GFO optimizers only.

        Runs the optimizer on the sklearn tuning experiment.
        """
        from hyperactive.opt._adapters._gfo import _BaseGFOadapter

        if not isinstance(object_instance, _BaseGFOadapter):
            return None

        optimizer = object_instance

        # 1. define the experiment
        from sklearn.datasets import load_iris
        from sklearn.metrics import accuracy_score
        from sklearn.model_selection import KFold
        from sklearn.svm import SVC

        from hyperactive.experiment.integrations import SklearnCvExperiment

        X, y = load_iris(return_X_y=True)

        sklearn_exp = SklearnCvExperiment(
            estimator=SVC(),
            scoring=accuracy_score,
            cv=KFold(n_splits=3, shuffle=True),
            X=X,
            y=y,
        )

        # 2. set up the optimizer
        import numpy as np

        _config = {
            "search_space": {
                "C": np.array([0.01, 0.1, 1, 10]),
                "gamma": np.array([0.0001, 0.01, 0.1, 1, 10]),
            },
            "n_iter": 100,
            "experiment": sklearn_exp,
        }
        optimizer = optimizer.clone().set_params(**_config)

        # 3. run the HillClimbing optimizer
        optimizer.solve()

        best_params = optimizer.best_params_
        assert best_params is not None, "Best parameters should not be None"
        assert isinstance(best_params, dict), "Best parameters should be a dictionary"
        assert "C" in best_params, "Best parameters should contain 'C'"
        assert "gamma" in best_params, "Best parameters should contain 'gamma'"

    def test_selection_direction_backend(self, object_instance):
        """Backends return argmax over standardized scores on controlled setup.

        This verifies the maximization direction using a tiny, deterministic
        experiment and a deliberately poor warm start. It is scoped per-backend
        to avoid brittle stochastic behavior.
        """
        # Import backend bases to check optimizer type
        from hyperactive.opt._adapters._base_optuna_adapter import _BaseOptunaAdapter
        from hyperactive.opt._adapters._gfo import _BaseGFOadapter
        from hyperactive.opt.gridsearch._sk import GridSearchSk
        from hyperactive.opt.random_search import RandomSearchSk

        # small helper to attach the right space key without per-estimator branching
        def _cfg_with_space(est, exp, space):
            cfg = {"experiment": exp}
            param_keys = set(est.get_params().keys())
            for key in (
                "param_space",
                "search_space",
                "param_grid",
                "param_distributions",
            ):
                if key in param_keys:
                    cfg[key] = space
                    break
            return cfg

        # set up a simple deterministic experiment with clear best vs worst
        from hyperactive.experiment.bench import Ackley

        exp = Ackley(d=2)
        # ackley is lower-better on evaluate; score flips sign to higher-better
        poor = {"x0": 4.0, "x1": 4.0}
        good = {"x0": 0.0, "x1": 0.0}

        # Helper: assert that returned params equal the known good point
        def _assert_good(best_params):
            assert isinstance(best_params, dict)
            assert best_params == good, (
                f"Optimizer should select argmax of standardized score. "
                f"Expected {good}, got {best_params}."
            )

        space = {"x0": [0.0, 4.0], "x1": [0.0, 4.0]}
        base_cfg = _cfg_with_space(object_instance, exp, space)

        # Optuna adapters: use warm_start via initialize and categorical space
        if isinstance(object_instance, _BaseOptunaAdapter):
            inst = object_instance.clone().set_params(
                **{
                    **base_cfg,
                    "n_trials": 2,
                    "initialize": {"warm_start": [poor, good]},
                    "random_state": 0,
                }
            )
            best_params = inst.solve()
            _assert_good(best_params)
            return None

        # GFO adapters: pass discrete space and warm_start; keep iterations tiny
        if isinstance(object_instance, _BaseGFOadapter):
            inst = object_instance.clone().set_params(
                **{
                    **base_cfg,
                    "n_iter": 2,
                    "initialize": {
                        "warm_start": [poor, good],
                        "grid": 0,
                        "random": 0,
                        "vertices": 0,
                    },
                    # keep Bayesian-style pre-sampling tiny to avoid heavy defaults
                    "random_state": 0,
                    "verbose": False,
                }
            )
            best_params = inst.solve()
            # In case the backend evaluates beyond warm starts, fall back to score check
            if best_params != good:
                sc_good, _ = exp.score(good)
                sc_poor, _ = exp.score(poor)
                sc_best, _ = exp.score(best_params)
                assert sc_best >= max(sc_good, sc_poor)
            else:
                _assert_good(best_params)
            return None

        # Sklearn GridSearch optimizer: test with discrete parameter grid
        if isinstance(object_instance, GridSearchSk):
            inst = object_instance.clone().set_params(**base_cfg)
            best_params = inst.solve()
            # GridSearchSk evaluates all grid combinations and selects best
            _assert_good(best_params)
            return None

        # Sklearn RandomSearch optimizer: test with discrete parameter distributions
        if isinstance(object_instance, RandomSearchSk):
            inst = object_instance.clone().set_params(
                **{
                    **base_cfg,
                    "n_iter": 4,  # Evaluate all combinations in small space
                    "random_state": 0,  # Ensure deterministic sampling
                }
            )
            best_params = inst.solve()
            # RandomSearchSk samples and selects best from evaluated points
            _assert_good(best_params)
            return None

        # For other backends, no-op here; targeted direction tests live elsewhere
        return None
